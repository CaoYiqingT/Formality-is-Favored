{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画柱状图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置图片大小和像素\n",
    "plt.figure(figsize=(8,6),dpi=80)\n",
    "\n",
    "# setting the datas\n",
    "y1 = [531,\n",
    "541,\n",
    "507,\n",
    "489,\n",
    "514,\n",
    "500]\n",
    "y2 = [469,\n",
    "459,\n",
    "493,\n",
    "511,\n",
    "486,\n",
    "500]\n",
    "\n",
    "x = np.arange(len(y1))\n",
    "\n",
    "# 设置柱体宽度\n",
    "total_width, n = 0.7, 2\n",
    "width = total_width / n\n",
    "xticks_labels = [\"birth date\", \"birth place\", \"company\", \"major\", \"university\", \"workplace\"]\n",
    "yticks_labels = [0, 100, 200, 300, 400, 500, 600, 700]\n",
    "\n",
    "plt.barh(x - width/2, y1, height=width, fc=\"limegreen\", label=\"general\")\n",
    "plt.barh(x + width/2, y2, height=width, fc=\"aqua\", label=\"with spell/grammar error\")\n",
    "plt.yticks(x, xticks_labels)\n",
    "plt.xticks(yticks_labels, yticks_labels)\n",
    "# 设置标题\n",
    "plt.title(\"General vs Spell/Grammar error\",fontsize=16)\n",
    "\n",
    "# 设置坐标轴名称\n",
    "# plt.xlabel(\"features\")\n",
    "plt.xlabel(\"Sample size statistics\")\n",
    "\n",
    "# 设置图注\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置图片大小和像素\n",
    "plt.figure(figsize=(20,6),dpi=80)\n",
    "\n",
    "x = np.arange(6)\n",
    "\n",
    "# 设置柱体宽度\n",
    "total_width, n = 0.7, 2\n",
    "width = total_width / n\n",
    "xticks_labels = [\"birth date\", \"birth place\", \"company\", \"major\", \"university\", \"workplace\"]\n",
    "yticks_labels = [0, 100, 200, 300, 400, 500, 600, 700, 800]\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "y1 = [307,472,510,419,362,481]\n",
    "y2 = [693,528,490,581,638,519]\n",
    "plt.barh(x - width/2, y1, height=width, fc=\"limegreen\", label=\"Novels\")\n",
    "plt.barh(x + width/2, y2, height=width, fc=\"aqua\", label=\"Newspapers\")\n",
    "plt.yticks(x, xticks_labels)\n",
    "plt.xticks(yticks_labels, yticks_labels)\n",
    "# 设置图注\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "y2 = [695,\n",
    "515,\n",
    "548,\n",
    "480,\n",
    "583,\n",
    "525]\n",
    "y1 = [305,\n",
    "485,\n",
    "452,\n",
    "520,\n",
    "417,\n",
    "475]\n",
    "plt.barh(x - width/2, y1, height=width, fc=\"limegreen\", label=\"Social Media\")\n",
    "plt.barh(x + width/2, y2, height=width, fc=\"aqua\", label=\"Novels\")\n",
    "# plt.yticks(x, xticks_labels)\n",
    "plt.xticks(yticks_labels, yticks_labels)\n",
    "# 设置图注\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "y1 = [481,\n",
    "517,\n",
    "491,\n",
    "511,\n",
    "375,\n",
    "480]\n",
    "y2 = [519,\n",
    "483,\n",
    "509,\n",
    "489,\n",
    "625,\n",
    "520]\n",
    "plt.barh(x - width/2, y1, height=width, fc=\"limegreen\", label=\"Scientific Reports\")\n",
    "plt.barh(x + width/2, y2, height=width, fc=\"aqua\", label=\"Newspapers\")\n",
    "plt.yticks(x, xticks_labels)\n",
    "plt.xticks(yticks_labels, yticks_labels)\n",
    "# 设置图注\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "y1 = [229,\n",
    "501,\n",
    "484,\n",
    "510,\n",
    "374,\n",
    "485]\n",
    "y2 = [771,\n",
    "499,\n",
    "516,\n",
    "490,\n",
    "626,\n",
    "515]\n",
    "plt.barh(x - width/2, y1, height=width, fc=\"limegreen\", label=\"Scientific Reports\")\n",
    "plt.barh(x + width/2, y2, height=width, fc=\"aqua\", label=\"Novels\")\n",
    "plt.yticks(x, xticks_labels)\n",
    "plt.xticks(yticks_labels, yticks_labels)\n",
    "# 设置图注\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "y1 = [483,\n",
    "508,\n",
    "493,\n",
    "496,\n",
    "407,\n",
    "488]\n",
    "y2 = [517,\n",
    "492,\n",
    "507,\n",
    "504,\n",
    "593,\n",
    "512]\n",
    "plt.barh(x - width/2, y1, height=width, fc=\"limegreen\", label=\"Scientific Reports\")\n",
    "plt.barh(x + width/2, y2, height=width, fc=\"aqua\", label=\"Social Media\")\n",
    "plt.yticks(x, xticks_labels)\n",
    "plt.xticks(yticks_labels, yticks_labels)\n",
    "# 设置图注\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "y1 = [245,\n",
    "465,\n",
    "502,\n",
    "452,\n",
    "319,\n",
    "483]\n",
    "y2 = [755,\n",
    "535,\n",
    "498,\n",
    "548,\n",
    "681,\n",
    "517]\n",
    "plt.barh(x - width/2, y1, height=width, fc=\"limegreen\", label=\"Social Media\")\n",
    "plt.barh(x + width/2, y2, height=width, fc=\"aqua\", label=\"Newspapers\")\n",
    "plt.yticks(x, xticks_labels)\n",
    "plt.xticks(yticks_labels, yticks_labels)\n",
    "# 设置图注\n",
    "plt.legend()\n",
    "\n",
    "# 设置标题\n",
    "# plt.title(\"General vs Spell/Grammar error\",fontsize=16)\n",
    "\n",
    "# 设置坐标轴名称\n",
    "# plt.xlabel(\"features\")\n",
    "plt.xlabel(\"Sample size statistics\")\n",
    "\n",
    "# 设置图注\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-30 23:19:49,328] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import LlamaModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIGH_DIM_VECS = {\n",
    "#     \"Blogs\": [],\n",
    "#     \"Personal Interviews\": [],\n",
    "#     \"Tabloids\": [],\n",
    "#     \"Textbooks\": [],\n",
    "#     \"Forum discussions\": [],\n",
    "#     \"Newspapers\": [],\n",
    "#     \"Social media\": [],\n",
    "#     \"Wikipedia\": [],\n",
    "#     \"Scientific reports\": [],\n",
    "#     \"Novels\": []\n",
    "# }\n",
    "# TYPE_LIST = [\"Scientific_reports\", \"Novels\", \"Forum_discussions\", \"Social_media\", \"Newspapers\", \"Wikipedia\", \"Blogs\", \"Personal_Interviews\", \"Textbooks\", \"Tabloids\"]\n",
    "HIGH_DIM_VECS = {\n",
    "    \"Newspapers\": [],\n",
    "    \"Social media\": [],\n",
    "    \"Scientific reports\": [],\n",
    "    \"Novels\": []\n",
    "}\n",
    "TYPE_LIST = [\"Scientific_reports\", \"Novels\", \"Social_media\", \"Newspapers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2818ac90982645609c6aa71393bab0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "100%|██████████| 1000/1000 [06:51<00:00,  2.43it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2880b10b7748449b612370fe02e73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:54<00:00,  2.41it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9748f83428ac4a9389bc377f68b8fd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:58<00:00,  2.39it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defec072d0424d9dabe5ae9b544448cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:56<00:00,  2.40it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c0952d95b844c58e97d88020015b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:55<00:00,  2.40it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e5b989a23f446187ce1f67099b847a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:57<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# first, let's see the birth_date information~\n",
    "def get_high_dim_vectors(model_name_or_path, test_file_path):\n",
    "    model = LlamaModel.from_pretrained(model_name_or_path, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    model.eval()\n",
    "\n",
    "    with open(test_file_path, 'r', encoding='utf8') as f:\n",
    "        total_data = json.load(f)\n",
    "    for data_item in tqdm(total_data):\n",
    "        text_list = data_preprocess(data_item, tokenizer)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for text_item in text_list:\n",
    "                output = model(text_item[\"input_ids\"])\n",
    "                HIGH_DIM_VECS[text_item[\"style\"]].append(output.last_hidden_state[0][text_item[\"birthday_start_pos\"]:])\n",
    "    \n",
    "def data_preprocess(data_item, tokenizer):\n",
    "    first_birth_date = data_item[\"first_type_info\"][\"birth_date\"]\n",
    "    second_birth_date = data_item[\"second_type_info\"][\"birth_date\"]\n",
    "\n",
    "    text_list = []\n",
    "\n",
    "    for text in data_item[\"text_result\"]:\n",
    "        if first_birth_date in text:\n",
    "            key_word = first_birth_date\n",
    "            style = data_item[\"first_type_info\"][\"type_name\"]\n",
    "        elif second_birth_date in text:\n",
    "            key_word = second_birth_date\n",
    "            style = data_item[\"second_type_info\"][\"type_name\"]\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        pre_text = text.split(key_word)[0].strip()\n",
    "        post_text = pre_text + ' ' + key_word\n",
    "\n",
    "        pre_input_ids = tokenizer(pre_text, return_tensors='pt').input_ids\n",
    "        post_input_ids = tokenizer(post_text, return_tensors='pt').input_ids\n",
    "\n",
    "        input_ids = post_input_ids\n",
    "        start_pos = len(pre_input_ids[0])\n",
    "\n",
    "        text_list.append({\n",
    "            \"style\": style,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"birthday_start_pos\": start_pos\n",
    "        })\n",
    "\n",
    "    return text_list\n",
    "\n",
    "def High_Dim_Vec_preprocess():\n",
    "    for key in list(HIGH_DIM_VECS.keys()):\n",
    "        HIGH_DIM_VECS[key] = torch.cat(HIGH_DIM_VECS[key], dim=0)\n",
    "\n",
    "def use_sne():\n",
    "    High_Dim_Vec_preprocess()\n",
    "    \n",
    "    input_vec = torch.cat([\n",
    "        # HIGH_DIM_VECS[\"Blogs\"], \n",
    "        # HIGH_DIM_VECS[\"Personal Interviews\"],\n",
    "        # HIGH_DIM_VECS[\"Tabloids\"],\n",
    "        # HIGH_DIM_VECS[\"Textbooks\"],\n",
    "        # HIGH_DIM_VECS[\"Forum discussions\"],\n",
    "        HIGH_DIM_VECS[\"Newspapers\"],\n",
    "        HIGH_DIM_VECS[\"Social media\"],\n",
    "        # HIGH_DIM_VECS[\"Wikipedia\"],\n",
    "        HIGH_DIM_VECS[\"Scientific reports\"],\n",
    "        HIGH_DIM_VECS[\"Novels\"],\n",
    "    ], dim=0)\n",
    "\n",
    "    labels = torch.cat([\n",
    "        # torch.ones(len(HIGH_DIM_VECS[\"Blogs\"])) * 1,\n",
    "        # torch.ones(len(HIGH_DIM_VECS[\"Personal Interviews\"])) * 2,\n",
    "        # torch.ones(len(HIGH_DIM_VECS[\"Tabloids\"])) * 3,\n",
    "        # torch.ones(len(HIGH_DIM_VECS[\"Textbooks\"])) * 4,\n",
    "        # torch.ones(len(HIGH_DIM_VECS[\"Forum discussions\"])) * 5,\n",
    "        torch.ones(len(HIGH_DIM_VECS[\"Newspapers\"])) * 1,\n",
    "        torch.ones(len(HIGH_DIM_VECS[\"Social media\"])) * 2,\n",
    "        # torch.ones(len(HIGH_DIM_VECS[\"Wikipedia\"])) * 8,\n",
    "        torch.ones(len(HIGH_DIM_VECS[\"Scientific reports\"])) * 3,\n",
    "        torch.ones(len(HIGH_DIM_VECS[\"Novels\"])) * 4,\n",
    "    ], dim=0)\n",
    "\n",
    "    tsne = TSNE(n_components=2, learning_rate=100, perplexity=10).fit_transform(input_vec)\n",
    "\n",
    "    plt.scatter(tsne[:, 0], tsne[:, 1], c=labels)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    for i in range(len(TYPE_LIST)):\n",
    "        for j in range(i + 1, len(TYPE_LIST)):\n",
    "            type_i = TYPE_LIST[i]\n",
    "            type_j = TYPE_LIST[j]\n",
    "\n",
    "            model_path = \"/opt/tiger/fake_arnold/{}_vs_{}/checkpoint-780\".format(type_i, type_j)\n",
    "            data_path = \"./data_scripts/type_fights/bio_data_train_{}_vs_{}.json\".format(type_i, type_j)\n",
    "\n",
    "            get_high_dim_vectors(model_path, data_path)\n",
    "\n",
    "    # use_sne()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "Random = random.Random(666)\n",
    "\n",
    "SAMPLE_HIGH_DIM_VECS = {}\n",
    "\n",
    "for key in list(HIGH_DIM_VECS.keys()):\n",
    "    origin_list = HIGH_DIM_VECS[key]\n",
    "    Random.shuffle(origin_list)\n",
    "    SAMPLE_HIGH_DIM_VECS[key] = origin_list[:500]\n",
    "\n",
    "torch.save(SAMPLE_HIGH_DIM_VECS, \"temp_result.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Newspapers', 'Social media', 'Scientific reports', 'Novels'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_HIGH_DIM_VECS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(SAMPLE_HIGH_DIM_VECS.keys()):\n",
    "        SAMPLE_HIGH_DIM_VECS[key] = torch.cat(SAMPLE_HIGH_DIM_VECS[key], dim=0)\n",
    "\n",
    "\n",
    "input_vec = torch.cat([\n",
    "    SAMPLE_HIGH_DIM_VECS[\"Newspapers\"],\n",
    "    SAMPLE_HIGH_DIM_VECS[\"Social media\"],\n",
    "    SAMPLE_HIGH_DIM_VECS[\"Scientific reports\"],\n",
    "    SAMPLE_HIGH_DIM_VECS[\"Novels\"],\n",
    "], dim=0)\n",
    "\n",
    "labels = torch.cat([\n",
    "    torch.ones(len(SAMPLE_HIGH_DIM_VECS[\"Newspapers\"])) * 1,\n",
    "    torch.ones(len(SAMPLE_HIGH_DIM_VECS[\"Social media\"])) * 2,\n",
    "    torch.ones(len(SAMPLE_HIGH_DIM_VECS[\"Scientific reports\"])) * 3,\n",
    "    torch.ones(len(SAMPLE_HIGH_DIM_VECS[\"Novels\"])) * 4,\n",
    "], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, learning_rate=100, perplexity=30).fit_transform(input_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne[:, 0], tsne[:, 1], c=labels, s=2)\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 折线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = [\"14m\", \"70m\", \"160m\", \"410m\", \"1b\", \"1.4b\", \"2.8b\", \"6.9b\"]\n",
    "x = range(0, 8)\n",
    "y = [450, 500, 550, 600, 650, 700, 750, 800, 900, 1000]\n",
    "y_labels = [\"45%\", \"50%\", \"55%\", \"60%\", \"65%\", \"70%\", \"75%\", \"80%\", \"90%\", \"100%\"]\n",
    "# 先只写newspaper的比例\n",
    "prompt_type=\"statement\"\n",
    "\n",
    "data_collect = {\n",
    "\"birth_date\" : [],\n",
    "\"birth_place\" : [],\n",
    "\"company\" : [],\n",
    "\"major\" : [],\n",
    "\"university\" : [],\n",
    "\"workplace\" : [],\n",
    "\"avg\" : [],\n",
    "}\n",
    "\n",
    "for feature_type in [\"birth_date\", \"birth_place\", \"company\", \"major\", \"university\", \"workplace\"]:\n",
    "    for size in model_size:\n",
    "        with open(\"./pythia_{}_Social_media_vs_Newspapers_ppl_acc_{}_{}.json\".format(size, feature_type, prompt_type), 'r', encoding='utf8') as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        data_collect[feature_type].append(result[\"choice Newspapers\"])\n",
    "\n",
    "plt.figure(figsize=(9,5.562),dpi=80)\n",
    "plt.title(\"Variation of model preferences with model scale\")\n",
    "plt.xticks(x, model_size)\n",
    "plt.yticks(y, y_labels)\n",
    "\n",
    "plt.plot(x, data_collect[\"birth_date\"], label='birth date')\n",
    "plt.plot(x, data_collect[\"birth_place\"], label='birth place')\n",
    "plt.plot(x, data_collect[\"company\"], label='company')\n",
    "plt.plot(x, data_collect[\"major\"], label='major')\n",
    "plt.plot(x, data_collect[\"university\"], label='university')\n",
    "plt.plot(x, data_collect[\"workplace\"], label='work place')\n",
    "# plt.plot(x, avg, label=\"avg\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
